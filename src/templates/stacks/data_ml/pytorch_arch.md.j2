# PyTorch Project Architecture

## ğŸ—ï¸ High-Level Overview

This project is built using **PyTorch 2.0+**.
It follows a **Research-to-Production** workflow:
1.  **Exploration:** Done in `notebooks/`.
2.  **Core Logic:** Defined in `src/` (Models, Datasets, Training Loops).
3.  **Configuration:** Hyperparameters are decoupled from code.

## ğŸ“‚ Directory Structure

```text
root/
â”œâ”€â”€ configs/             # Configuration files (YAML/JSON)
â”‚   â”œâ”€â”€ model/           # Model-specific configs
â”‚   â”œâ”€â”€ train/           # Training hyperparameters
â”‚   â””â”€â”€ config.yaml      # Main entry config
â”œâ”€â”€ data/                # Data storage (gitignored)
â”‚   â”œâ”€â”€ raw/             # Original immutable data
â”‚   â”œâ”€â”€ processed/       # Cleaned/Tokenized data
â”‚   â””â”€â”€ external/        # Data from third-party sources
â”œâ”€â”€ notebooks/           # Jupyter Notebooks for EDA & Prototyping
â”‚   â”œâ”€â”€ 01_eda.ipynb
â”‚   â””â”€â”€ 02_model_test.ipynb
â”œâ”€â”€ src/                 # Source Code
â”‚   â”œâ”€â”€ data/            # Data Loading Logic
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ dataset.py   # Custom Dataset classes
â”‚   â”‚   â””â”€â”€ make_dataset.py
â”‚   â”œâ”€â”€ models/          # Model Architectures
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py      # Abstract Base Model
â”‚   â”‚   â””â”€â”€ unet.py      # Specific Implementations
â”‚   â”œâ”€â”€ training/        # Training & Evaluation Loops
â”‚   â”‚   â”œâ”€â”€ trainer.py   # Trainer Class
â”‚   â”‚   â””â”€â”€ metrics.py   # Custom Metrics
â”‚   â””â”€â”€ utils/           # Helper functions (Seeding, Logging)
â”œâ”€â”€ train.py             # Training Entry Point
â”œâ”€â”€ inference.py         # Inference Entry Point
â””â”€â”€ requirements.txt
```

## ğŸ§© Key Architectural Patterns

### 1. Model vs. Trainer Separation
- **Model (`src/models/`):** Pure `nn.Module` classes. They only handle the forward pass.
- **Trainer (`src/training/`):** Handles the training loop, validation loop, checkpointing, and logging.
- **Benefit:** Allows you to swap model architectures without rewriting the training logic.

### 2. Data Pipeline
- **Dataset:** Inherit from `torch.utils.data.Dataset`. strictly implement `__len__` and `__getitem__`.
- **DataLoader:** Wraps the dataset for batching and shuffling.
- **Transforms:** Use `torchvision.transforms` (or Albumentations) defined in the configuration, not hardcoded in the Dataset.

### 3. Configuration Management
- We avoid hardcoding hyperparameters (LR, Batch Size) in Python files.
- All parameters are loaded from `configs/` or CLI arguments at runtime.

### 4. PyTorch 2.0 Compilation
- The architecture supports `torch.compile()`.
- Models are designed to be "Graph Friendly" (avoiding dynamic control flow where possible) to leverage inductor optimization.

## ğŸ“Š Experiment Tracking
- Checkpoints are saved to `models/checkpoints/` with a timestamp or experiment ID.
- Metrics (Loss, Accuracy) are logged to `logs/` (compatible with TensorBoard or MLFlow).