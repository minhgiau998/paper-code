# Scikit-learn Project Architecture

## ðŸ—ï¸ High-Level Overview

This project is built using **Scikit-learn** for classical machine learning tasks.
It focuses on **Reproducibility** and **Deployment-Readiness** by strictly encapsulating logic into Pipelines.

## ðŸ“‚ Directory Structure

We move away from single-script workflows to a structured package.

```text
root/
â”œâ”€â”€ configs/             # YAML configurations for models/grids
â”‚   â”œâ”€â”€ grid_search.yaml
â”‚   â””â”€â”€ model_params.yaml
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/             # Immutable raw data
â”‚   â””â”€â”€ processed/       # Training-ready data (optional)
â”œâ”€â”€ models/              # Serialized models (.joblib)
â”œâ”€â”€ notebooks/           # EDA and Prototyping
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ features/        # Custom Transformers
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ transformers.py # Custom classes inheriting BaseEstimator
â”‚   â”œâ”€â”€ pipeline/        # Pipeline Construction
â”‚   â”‚   â””â”€â”€ build.py     # Factory to assemble ColumnTransformers
â”‚   â”œâ”€â”€ training/        # Training Logic
â”‚   â”‚   â”œâ”€â”€ train.py     # Main training loop
â”‚   â”‚   â””â”€â”€ evaluate.py  # CV and Metric reporting
â”‚   â””â”€â”€ visualization/   # Plotting utilities
â”œâ”€â”€ train.py             # CLI Entry point
â””â”€â”€ requirements.txt
```

## ðŸ§© Key Architectural Patterns

### 1. The Pipeline First Approach
**Rule:** Never manually preprocess data (scale, encode, impute) outside of a Pipeline.
- **Why?** It prevents **Data Leakage** (calculating mean on the whole dataset instead of just train set) and ensures the exact same transformations are applied during inference.
- **Component:** Use `sklearn.pipeline.Pipeline` and `compose.ColumnTransformer`.

### 2. Custom Transformers
- Domain-specific logic (e.g., "Extract text length", "Date decomposition") should be encapsulated in classes inheriting from `BaseEstimator` and `TransformerMixin`.
- These transformers are then plugged into the main Pipeline.

### 3. Hyperparameter Configuration
- Model parameters (Random Forest depth, XGBoost learning rate) are defined in `configs/`, decoupled from the code.
- This allows running experiments without changing source code.

### 4. Model Persistence
- The final artifact is the **Entire Pipeline** (Preprocessing + Model).
- We use **Joblib** for efficient serialization of large numpy arrays inside the model.

## ðŸ“Š Experiment Tracking
{% if libraries | default([]) | select("in", ["MLflow"]) | list | length > 0 %}
- **MLflow** is used to track runs, parameters, and metrics.
- Artifacts (plots, models) are logged to the MLflow server.
{% else %}
- Metrics are logged to `logs/metrics.json`.
- Confusion matrices and ROC curves are saved to `reports/figures/`.
{% endif %}